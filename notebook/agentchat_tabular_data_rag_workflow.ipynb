{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic RAG workflow on tabular data from a PDF file\n",
    "\n",
    "In this notebook, we're building a workflow to extract accurate tabular data information from a pdf file.\n",
    "\n",
    "The following bullets summarize the notebook, with highlights being:\n",
    "\n",
    "- Parse the pdf file and extract tables into images.\n",
    "- A single rag agent fails to get the accurate information from tabular data.\n",
    "- An agentic workflow using a groupchat to extract accurate information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{=mdx}\n",
    ":::info Requirements\n",
    "Unstructured-IO is a dependency for this notebook to parse the pdf. Please install the following dependencies\n",
    "# Install Poppler https://pdf2image.readthedocs.io/en/latest/installation.html\n",
    "# Install Tesseract https://tesseract-ocr.github.io/tessdoc/Installation.html\n",
    "# %pip install unstructured==0.16.11, pi-heif==0.21.0, unstructured_inference==0.8.1, unstructured.pytesseract==0.3.13, pytesseract==0.3.13\n",
    ":::\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Configuration and OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4o\"],\n",
    "    },\n",
    ")\n",
    "os.environ[\"OPENAI_API_KEY\"] = config_list[0][\"api_key\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse PDF file\n",
    "\n",
    "**Skip and use parsed files to run the rest.**\n",
    "This step is expensive and time consuming, please skip unless you insist.\n",
    "\n",
    "For the notebook, we use a common finanical document, [Nvidia 2024 10-K](https://investor.nvidia.com/financial-info/sec-filings/sec-filings-details/default.aspx?FilingId=17293267) as an example ([file download link](https://d18rn0p25nwr6d.cloudfront.net/CIK-0001045810/1cbe8fe7-e08a-46e3-8dcc-b429fc06c1a4.pdf)).\n",
    "\n",
    "We use Unstructured-IO to parse the PDF, the table and image from the pdf are extracted out as .jpg files.\n",
    "\n",
    "All parsed output are saved in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.staging.base import elements_to_json\n",
    "\n",
    "file_elements = partition_pdf(\n",
    "    filename=\"./input_files/nvidia_10k_2024.pdf\",\n",
    "    strategy=\"hi_res\",\n",
    "    languages=[\"eng\"],\n",
    "    infer_table_structure=True,\n",
    "    extract_images_in_pdf=True,\n",
    "    extract_image_block_output_dir=\"./parsed_pdf_info\",\n",
    "    extract_image_block_types=[\"Image\", \"Table\"],\n",
    "    extract_forms=False,\n",
    "    form_extraction_skip_tables=False,\n",
    ")\n",
    "\n",
    "elements_to_json(elements=file_elements, filename=\"parsed_elements.json\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "output_elements = []\n",
    "keys_to_extract = [\"element_id\", \"text\", \"type\"]\n",
    "metadata_keys = [\"page_number\", \"parent_id\", \"image_path\"]\n",
    "text_types = set([\"Text\", \"UncategorizedText\", \"NarrativeText\"])\n",
    "element_length = len(file_elements)\n",
    "for idx in range(element_length):\n",
    "    data = file_elements[idx].to_dict()\n",
    "    new_data = {key: data[key] for key in keys_to_extract}\n",
    "    metadata = data[\"metadata\"]\n",
    "    for key in metadata_keys:\n",
    "        if key in metadata:\n",
    "            new_data[key] = metadata[key]\n",
    "    if data[\"type\"] == \"Table\":\n",
    "        if idx > 0:\n",
    "            pre_data = file_elements[idx - 1].to_dict()\n",
    "            if pre_data[\"type\"] in text_types:\n",
    "                new_data[\"text\"] = pre_data[\"text\"] + new_data[\"text\"]\n",
    "        if idx < element_length - 1:\n",
    "            post_data = file_elements[idx + 1].to_dict()\n",
    "            if post_data[\"type\"] in text_types:\n",
    "                new_data[\"text\"] = new_data[\"text\"] + post_data[\"text\"]\n",
    "    output_elements.append(new_data)\n",
    "\n",
    "with open(\"proessed_elements.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(output_elements, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "# This is needed to allow nested asyncio calls for Neo4j in Jupyter\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from autogen import AssistantAgent, ConversableAgent, UserProxyAgent\n",
    "\n",
    "# load documents\n",
    "from autogen.agentchat.contrib.graph_rag.document import Document, DocumentType\n",
    "from autogen.agentchat.contrib.graph_rag.neo4j_graph_query_engine import Neo4jGraphQueryEngine\n",
    "from autogen.agentchat.contrib.graph_rag.neo4j_graph_rag_capability import Neo4jGraphCapability\n",
    "from autogen.agentchat.contrib.multimodal_conversable_agent import MultimodalConversableAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a knowledge graph with sample data\n",
    "\n",
    "To save time, we uses a small subset of the data for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c02a975b784c5db13ea02163bd140a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting paths from text with schema: 100%|██████████| 1/1 [00:06<00:00,  6.96s/it]\n",
      "Extracting and inferring knowledge graph from text: 100%|██████████| 1/1 [00:06<00:00,  6.77s/it]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  3.30it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.54s/it]\n"
     ]
    }
   ],
   "source": [
    "input_path = \"./agentchat_pdf_rag/sample_elements.json\"\n",
    "input_documents = [\n",
    "    Document(doctype=DocumentType.JSON, path_or_url=input_path),\n",
    "]\n",
    "\n",
    "query_engine = Neo4jGraphQueryEngine(\n",
    "    username=\"neo4j\",  # Change if you reset username\n",
    "    password=\"password\",  # Change if you reset password\n",
    "    host=\"bolt://172.17.0.3\",  # Change\n",
    "    port=7687,  # if needed\n",
    "    llm=OpenAI(model=\"gpt-4o\", temperature=0.0),  # Default, no need to specify\n",
    "    embedding=OpenAIEmbedding(model_name=\"text-embedding-3-small\"),  # except you want to use a different model\n",
    "    database=\"neo4j\",  # Change if you want to store the graphh in your custom database\n",
    ")\n",
    "\n",
    "# query_engine._clear()\n",
    "# Ingest data and create a new property graph\n",
    "query_engine.init_db(input_doc=input_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to knowledge graph if it is built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = Neo4jGraphQueryEngine(\n",
    "    username=\"neo4j\",\n",
    "    password=\"password\",\n",
    "    host=\"bolt://172.17.0.3\",\n",
    "    port=7687,\n",
    "    database=\"neo4j\",\n",
    ")\n",
    "\n",
    "query_engine.connect_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native RAG Agent Solution\n",
    "\n",
    "The following shows that when use a native rag agent for parsed data, the agent failed to get the right information (5,282 instead of 4,430).\n",
    "\n",
    "Our best guess is that rag agent fails to understand the table structure from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to nvidia_rag):\n",
      "\n",
      "Could you list all tables from the document and its image_path?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mnvidia_rag\u001b[0m (to user_proxy):\n",
      "\n",
      "Certainly! Here is the table from the document along with its image path:\n",
      "\n",
      "1. Table: NVIDIA Corporation and Subsidiaries Consolidated Balance Sheets (In millions, except value)\n",
      "   - Image Path: ./parsed_pdf_info/table-52-17.jpg\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to nvidia_rag):\n",
      "\n",
      "what is goodwill asset (in millions) for 2024?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mnvidia_rag\u001b[0m (to user_proxy):\n",
      "\n",
      "The goodwill asset for NVIDIA Corporation in 2024 is $5,282 million.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': 'Could you list all tables from the document and its image_path?', 'role': 'assistant', 'name': 'user_proxy'}, {'content': 'Certainly! Here is the table from the document along with its image path:\\n\\n1. Table: NVIDIA Corporation and Subsidiaries Consolidated Balance Sheets (In millions, except value)\\n   - Image Path: ./parsed_pdf_info/table-52-17.jpg', 'role': 'user', 'name': 'nvidia_rag'}, {'content': 'what is goodwill asset (in millions) for 2024?', 'role': 'assistant', 'name': 'user_proxy'}, {'content': 'The goodwill asset for NVIDIA Corporation in 2024 is $5,282 million.', 'role': 'user', 'name': 'nvidia_rag'}], summary='The goodwill asset for NVIDIA Corporation in 2024 is $5,282 million.', cost={'usage_including_cached_inference': {'total_cost': 0}, 'usage_excluding_cached_inference': {'total_cost': 0}}, human_input=['what is goodwill asset (in millions) for 2024?', 'exit'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_agent = ConversableAgent(\n",
    "    name=\"nvidia_rag\",\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# Associate the capability with the agent\n",
    "graph_rag_capability = Neo4jGraphCapability(query_engine)\n",
    "graph_rag_capability.add_to_agent(rag_agent)\n",
    "\n",
    "# Create a user proxy agent to converse with our RAG agent\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"ALWAYS\",\n",
    ")\n",
    "\n",
    "user_proxy.initiate_chat(rag_agent, message=\"Could you list all tables from the document and its image_path?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agentic RAG workflow for tabular data\n",
    "From the above example, when asked the goodwill asset (in millions) of the table NVIDIA Corporation and Subsidiaries Consolidated Balance Sheets, the answer was wrong.\n",
    "The correct figure from the table is $4,430 million instead of $4,400 million. \n",
    "To enhance the RAG performance from the tabular data, we introduce the enhanced workflow.\n",
    "\n",
    "The workflow consists a group of agent and use groupchat to coordinate. It breaks the RAG into 3 mains steps,\n",
    "1. it finds the parsed image of the corresponding table.\n",
    "2. it converts the image to table in structured Markdown format.\n",
    "3. With the table in Markdown, the workflow answer the question with the correct data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = {\n",
    "    \"cache_seed\": 42,  # change the cache_seed for different trials\n",
    "    \"temperature\": 1,\n",
    "    \"config_list\": config_list,\n",
    "    \"timeout\": 120,\n",
    "}\n",
    "\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"User_proxy\",\n",
    "    system_message=\"A human admin.\",\n",
    "    human_input_mode=\"ALWAYS\",  # Try between ALWAYS or NEVER\n",
    "    code_execution_config=False,\n",
    ")\n",
    "\n",
    "table_assistant = AssistantAgent(\n",
    "    name=\"table_assistant\",\n",
    "    system_message=\"\"\"You are a helpful assistant.\n",
    "    You will extract the table name from the message and reply with \"Find image_path for Table: {table_name}\".\n",
    "    For example, when you got message \"What is column data in table XYZ?\",\n",
    "    you will reply \"Find image_path for Table: XYZ\"\n",
    "    \"\"\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    ")\n",
    "\n",
    "rag_agent = ConversableAgent(\n",
    "    name=\"nvidia_rag\",\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# Associate the capability with the agent\n",
    "graph_rag_capability = Neo4jGraphCapability(query_engine)\n",
    "graph_rag_capability.add_to_agent(rag_agent)\n",
    "\n",
    "img_folder = \"/workspaces/ag2/notebook/agentchat_pdf_rag/parsed_pdf_info\"\n",
    "\n",
    "img_request_format = ConversableAgent(\n",
    "    name=\"img_request_format\",\n",
    "    system_message=f\"\"\"You are a helpful assistant.\n",
    "    You will extract the table_file_name from the message and reply with \"Please extract table from the following image and convert it to Markdown.\n",
    "    <img {img_folder}/table_file_name>.\".\n",
    "    For example, when you got message \"The image path for the table titled XYZ is \"./parsed_pdf_info/abcde\".\",\n",
    "    you will reply \"Please extract table from the following image and convert it to Markdown.\n",
    "    <img {img_folder}/abcde>.\"\n",
    "    \"\"\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "image2table_convertor = MultimodalConversableAgent(\n",
    "    name=\"image2table_convertor\",\n",
    "    system_message=\"\"\"\n",
    "    You are an image to table convertor. You will receive an image path of a table. The original table could be in csv, pdf or other format.\n",
    "    You need to the following step in sequence,\n",
    "    1. extract the table content and structure.\n",
    "    2. Make sure the structure is complete.\n",
    "    3. Correct typos in the text fields.\n",
    "    4. In the end, output the table in Markdown.\n",
    "    \"\"\",\n",
    "    llm_config={\"config_list\": config_list, \"max_tokens\": 300},\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=1,\n",
    ")\n",
    "\n",
    "conclusion = AssistantAgent(\n",
    "    name=\"conclusion\",\n",
    "    system_message=\"\"\"You are a helpful assistant.\n",
    "    Base on the history of the groupchat, answer the original question from User_proxy.\n",
    "    \"\"\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "What is goodwill asset (in millions) for 2024 in table NVIDIA Corporation and Subsidiaries Consolidated Balance Sheets?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: table_assistant\n",
      "\u001b[0m\n",
      "\u001b[33mtable_assistant\u001b[0m (to chat_manager):\n",
      "\n",
      "Find image_path for Table: NVIDIA Corporation and Subsidiaries Consolidated Balance Sheets\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: nvidia_rag\n",
      "\u001b[0m\n",
      "\u001b[33mnvidia_rag\u001b[0m (to chat_manager):\n",
      "\n",
      "The image path for the table titled \"NVIDIA Corporation and Subsidiaries Consolidated Balance Sheets\" is \"./parsed_pdf_info/table-52-17.jpg\".\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: img_request_format\n",
      "\u001b[0m\n",
      "\u001b[33mimg_request_format\u001b[0m (to chat_manager):\n",
      "\n",
      "Please extract table from the following image and convert it to Markdown.\n",
      "<img /workspaces/ag2/notebook/agentchat_pdf_rag/parsed_pdf_info/table-52-17.jpg>.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: image2table_convertor\n",
      "\u001b[0m\n",
      "\u001b[33mimage2table_convertor\u001b[0m (to chat_manager):\n",
      "\n",
      "Here is the table extracted in Markdown format:\n",
      "\n",
      "```markdown\n",
      "|                                      | Jan 28, 2024 | Jan 29, 2023 |\n",
      "|--------------------------------------|--------------|--------------|\n",
      "| **Assets**                           |              |              |\n",
      "| Current assets:                      |              |              |\n",
      "| &nbsp; &nbsp; Cash and cash equivalents     | $7,280        | $3,389        |\n",
      "| &nbsp; &nbsp; Marketable securities          | $18,704       | $9,907        |\n",
      "| &nbsp; &nbsp; Accounts receivable, net      | $9,999        | $3,827        |\n",
      "| &nbsp; &nbsp; Inventories                   | $5,282        | $5,159        |\n",
      "| &nbsp; &nbsp; Prepaid expenses and other current assets | $3,080        | $791          |\n",
      "| **Total current assets**            | $44,345       | $23,073       |\n",
      "| Property and equipment, net         | $3,914        | $3,807        |\n",
      "| Operating lease assets              | $1,346        | $1,038        |\n",
      "| **Goodwill**                        | $4,430        | $4,372        |\n",
      "| Intangible assets, net              | $1,112        | $1,676        |\n",
      "| Deferred income tax assets          | $6,081        | $3,396        |\n",
      "| Other\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: conclusion\n",
      "\u001b[0m\n",
      "\u001b[33mconclusion\u001b[0m (to chat_manager):\n",
      "\n",
      "Based on the table \"NVIDIA Corporation and Subsidiaries Consolidated Balance Sheets,\" the goodwill asset for the fiscal year ending January 28, 2024, is $4,430 million.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: User_proxy\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "groupchat = autogen.GroupChat(\n",
    "    agents=[\n",
    "        user_proxy,\n",
    "        table_assistant,\n",
    "        rag_agent,\n",
    "        img_request_format,\n",
    "        image2table_convertor,\n",
    "        conclusion,\n",
    "    ],\n",
    "    messages=[],\n",
    "    max_round=12,\n",
    "    speaker_selection_method=\"round_robin\",\n",
    ")\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
    "chat_result = user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message=\"What is goodwill asset (in millions) for 2024 in table NVIDIA Corporation and Subsidiaries Consolidated Balance Sheets?\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
